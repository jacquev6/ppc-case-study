from email import generator
import os
import re
import sys

import joblib
import matplotlib.pyplot as plt
import yaml


# def gen_report():
#     versions = list(range(6))
#     flavors = {
#         "seq": [
#             """\
#             Despite running at a comparable frequency, my CPU doesn't quite reach the 1.3 billion operations per second achieved by the author on Linux with `v1` for n=4000.
#             The performance I get is close to the one they get on macOS. They say the difference they observe between the two OSes is due to macOS not using Transparent Hugepage Support.
#             @todo Check page size on my system.""",
#             """\
#             The performance of `v0` decreases around n=1600, similarly to what the author observed, and linear reading in `v1` fixes that first bottleneck.
#             """
#         ],
#         "paral-4": [],
#         "paral-14": [],
#         "paral-28": [],
#     }

#     jobs = []

#     yield from textwrap.dedent("""\
#         <!-- WARNING, this file is generated by report.py. MANUAL CHANGES WILL BE LOST -->

#         This repository is just me following along [this great parallel optimization case study](https://ppc.cs.aalto.fi/ch2/). A huge THANK YOU to its author [Jukka Suomela](https://jukkasuomela.fi/).

#         Here are the graphs resulting from running different versions of the code, unaltered from the case study, on a 14-cores i9-10940X CPU.

#         The `seq` version was compiled without OpenMP support.
#         The `paral-`*N* versions were compiled with `-fopenmp` and run with `OMP_NUM_THREADS=`*N*.

#         CPU specs:
#         - 14 cores, 28 threads
#         - L1 cache: 896 KiB
#         - L2 cache: 14 MiB
#         - L3 cache: 19 MiB
#         - frequency: 1.2 GHz to 4.6 GHz; around 4.1 GHz to 4.3 GHz when running the benchmarks

#         Memory specs:
#         - 4 banks of 16 GiB
#         - DIMM DDR4 Synchronous 2133 MHz (0.5 ns)

#         You can use `./run.sh` to regenerate this report on your machine.
#         You may want to adapt the parallelism levels to your CPU, in the `Makefile` and in `report.py`.
#     """).splitlines()

#     for flavor, remarks in flavors.items():
#         yield f"# {flavor}"
#         yield f"![{flavor}]({flavor}.png)"
#         for remark in remarks:
#             yield from textwrap.dedent(remark).splitlines()
#             yield ""
#         jobs.append(joblib.delayed(make_graph)(flavor, versions, f"{flavor}.png"))

#     joblib.Parallel(n_jobs=-1)(jobs)


# def make_graph(flavor, versions, file_name):
#     fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8), layout="constrained")

#     for version in versions:
#         with open(f"build/v{version}-{flavor}.yml") as f:
#             benchmarks = yaml.load(f, Loader=yaml.Loader)
#             ns = []
#             durations = []
#             for benchmark in benchmarks:
#                 ns.append(benchmark["n"])
#                 durations.append(benchmark["duration"]["nanoseconds"] / 1e9)
#             ax1.plot(ns, durations, '.-', label=f"v{version}")
#             ax2.plot(ns, [2 * n ** 3 / duration / 1e9 for (n, duration) in zip(ns, durations)], ".-", label=f"v{version}")

#     ax1.set_xlim(left=0, right=5000)
#     ax1.set_xlabel("Size")
#     ax1.set_ylim(bottom=0)
#     ax1.set_ylabel("Duration (s)")
#     ax1.legend()

#     ax2.set_xlim(left=0, right=5000)
#     ax2.set_xlabel("Size")
#     ax2.set_ylim(bottom=0)
#     ax2.set_ylabel("Billions of useful operations per second")
#     ax2.legend()

#     fig.savefig(file_name, dpi=300)
#     plt.close(fig)


def main(file_name):
    image_regex = re.compile(r"!\[.*\]\((.*)\)")
    jobs = []
    with open(file_name) as f:
        for line in f:
            m = image_regex.fullmatch(line.rstrip())
            if m:
                jobs.append(joblib.delayed(generate)(m.group(1)))
    joblib.Parallel(n_jobs=-1)(jobs)


def generate(file_name):
    print(f"Generating {file_name}")

    base_name, ext = os.path.splitext(file_name)
    assert ext == ".png"
    if base_name.startswith("vsize-"):
        low_version, high_version = base_name[6:].split("-")
        low_version = int(low_version)
        high_version = int(high_version)

        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8), layout="constrained")

        for version in range(low_version, high_version + 1):
            with open(f"build/v{version}-seq.yml") as f:
                benchmarks = yaml.load(f, Loader=yaml.Loader)
                ns = []
                durations = []
                for benchmark in benchmarks:
                    ns.append(benchmark["n"])
                    durations.append(benchmark["duration"]["nanoseconds"] / 1e9)
                ax1.plot(ns, durations, '.-', label=f"v{version}", color=f"C{version}")
                ax2.plot(ns, [2 * n ** 3 / duration / 1e9 for (n, duration) in zip(ns, durations)], ".-", label=f"v{version}", color=f"C{version}")

        ax1.set_xlim(left=0, right=5000)
        ax1.set_xlabel("Size")
        ax1.set_ylim(bottom=0)
        ax1.set_ylabel("Duration (s)")
        ax1.legend()

        ax2.set_xlim(left=0, right=5000)
        ax2.set_xlabel("Size")
        ax2.set_ylim(bottom=0)
        ax2.set_ylabel("Billions of useful operations per second")
        ax2.legend()

        fig.savefig(file_name, dpi=300)
        plt.close(fig)
    else:
        assert False


if __name__ == "__main__":
    main(sys.argv[1])
